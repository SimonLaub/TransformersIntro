{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c5460e5-2601-49bc-a413-52de955adb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple RAG for QA about Document\n",
    "# Sila. Novermber 29th, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbd5c22d-8f2e-4fd2-9dbc-47c797561dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I do! Claude 3 is a popular and powerful virtual assistant software developed by the French company Xamarin. It allows users to control their home automation devices remotely with voice commands through their smartphone or tablet. The software uses deep learning technology to learn the user's voice patterns and respond appropriately, making it easy to use for beginners as well as experienced users. With Claude 3, users can set up smart home scenes, adjust room temperatures, turn on/off lights, start music, and even control appliances like washing machines or refrigerators!\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "LLM = \"tinyllama\"\n",
    "\n",
    "response = ollama.generate(model=LLM, prompt=\"Do you know about Claude 3?\")\n",
    "\n",
    "print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efdd2325-982e-49eb-95f2-01a05bacac90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ollama._types.EmbeddingsResponse"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model = \"nomic-embed-text\"\n",
    "\n",
    "embeds = ollama.embeddings(model=embedding_model, prompt=\"Do you know about Claude 3?\")\n",
    "\n",
    "type(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa9826b7-cc9e-4974-a1e6-8a5e6aa50d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.1646595597267151,\n",
       "  -0.6364230513572693,\n",
       "  -4.215642929077148,\n",
       "  -0.866477906703949,\n",
       "  -0.22994472086429596],\n",
       " 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds[\"embedding\"][:5], len(embeds[\"embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "303a5b8d-cf28-4e0b-afa3-5e564a874e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "687b4f9e-61a0-4dcd-9c8e-cd58b9033eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f3a8dab-0078-461d-98a0-d25f0159f9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d566bf7-3f51-47bd-87f4-d1d8cd26efa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79008a8c-bcb6-4543-b76f-0f7223c009c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r'C:\\Users\\sila\\PycharmProjects\\RAG_Supervisor\\Success.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc16611e-1a76-4b95-be98-d99a8b8855e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(directory):\n",
    "    loader = PyPDFLoader(directory)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "documents = load_docs(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ea50fa2-b2e6-4419-a694-677ac3394be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents(documents, chunk_size=500, chunk_overlap=20):\n",
    "    text_split = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = text_split.split_documents(documents)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5447fe11-8d48-474f-a990-19fda50b9d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 299\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sila\\AppData\\Local\\Temp\\ipykernel_17128\\634650621.py:4: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"nomic-embed-text\") # Later, replace with newer langchain code.\n"
     ]
    }
   ],
   "source": [
    "chunks = split_documents(documents)\n",
    "print(f\"Total number of chunks: {len(chunks)}\")\n",
    "print(\"\\n\")\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\") # Later, replace with newer langchain code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d19739b-376a-4152-82ec-34869d133336",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(chunks, embeddings, collection_name = \"local-rag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe05dfb5-acbf-4c68-8232-620b110ecacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e3b301e-030f-4ff5-9c0b-261527149c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question:\n",
    "If you don't know the answer, then answer from your own knowledge and dont give just one word answer, and dont tell the user that you are answering from your knowledge.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a0cd8b0-f4a4-481d-a41a-4d5d3765375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8df913bd-e58b-424b-8927-84b1bdbbddf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sila\\AppData\\Local\\Temp\\ipykernel_17128\\1478007701.py:2: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=local_model)\n"
     ]
    }
   ],
   "source": [
    "local_model = \"tinyllama\"\n",
    "llm = ChatOllama(model=local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9c388ab-cd53-4fbd-9567-0ca39c29505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "        {\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1b415c-e0f0-44d7-8b4f-31b550215a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Question:  What will the graduate have knowledge about?\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query = str(input(\"Enter Question: \"))\n",
    "    print(rag_chain.invoke(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de660738-f831-491b-ae7a-b323a2fc4a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sila\\PycharmProjects\\RAG_Supervisor\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16e39b27-784e-439a-8add-fa86d16097bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a Gradio frontend make sure you have run previous cells\n",
    "import gradio as gr\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=rag_chain.invoke,  #use the function we defined in a previous cell\n",
    "    inputs=\"text\", \n",
    "    outputs= \"text\"\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f98ad61-6f6f-452d-80b8-a3c6e6ee0428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
